# -*- coding: utf-8 -*-
"""ClassificationFinal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15U43NMlJ6hnw-b8vMsBbXL7nY3a0r0xy
"""

import torch
from torchvision import models, transforms
from PIL import Image
import json
import requests
from PIL import Image
from transformers import AutoModelForImageClassification, ViTFeatureExtractor
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
import torch
import torch.nn as nn
import torch.optim as optim
from transformers import ViTImageProcessor


# Download ImageNet class index
class_idx_url = 'https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json'
class_idx = requests.get(class_idx_url).json()

# Load the pre-trained model
model = models.resnet50(weights='ResNet50_Weights.IMAGENET1K_V1')
model.eval()

# Define a transform to preprocess the image
transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load an image
image_path = '/1163.jpg'  # Update this path to your image file
image = Image.open(image_path)


# Convert PIL Image to PyTorch tensor and add batch dimension
image_tensor = transform(image).unsqueeze(0)

# Predict with the model
model.eval()  # Set the model to evaluation mode
with torch.no_grad():
    outputs = model(image_tensor)
    probabilities = torch.nn.functional.softmax(outputs[0], dim=0)


# Get top 5 probabilities and indices
top5_prob, top5_catid = torch.topk(probabilities, 5)
for i in range(top5_prob.size(0)):
    print(f"{class_idx[top5_catid[i].item()]}: {top5_prob[i].item() * 100:.2f}%")



def classify_Image(imagePath):
  # Define a custom dataset for your classification task
  class CustomDataset(Dataset):
      def _init_(self, image_paths, labels, transform=None):
          self.image_paths = image_paths
          self.labels = labels
          self.transform = transform

      def _len_(self):
          return len(self.image_paths)

      def _getitem_(self, idx):
          image = Image.open(self.image_paths[idx]).convert("RGB")
          label = torch.tensor(self.labels[idx])

          if self.transform:
              image = self.transform(image)

          return image, label

  # Define your image transformation pipeline
  transform = transforms.Compose([
      transforms.Resize((224, 224)),
      transforms.ToTensor(),
  ])

  # Specify image paths and corresponding labels
  image_paths = [
      imagePath
      # Add more paths as needed
  ]

  labels = [0, 0, 1]  # Corresponding labels for each image path

  # Create a custom dataset
  dataset = CustomDataset(image_paths, labels, transform=transform)

  # Create a DataLoader for training
  batch_size = 32
  dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

  # Load the pre-trained model and feature extractor
  model = AutoModelForImageClassification.from_pretrained("google/vit-base-patch16-224")
  feature_extractor = ViTFeatureExtractor.from_pretrained("google/vit-base-patch16-224")
  feature_extractor = ViTImageProcessor.from_pretrained("google/vit-base-patch16-224")


  # Modify the classification head for your specific task
  model.classifier = nn.Linear(model.config.hidden_size, len(set(dataset.labels)))

  # Define loss function and optimizer
  criterion = nn.CrossEntropyLoss()
  optimizer = optim.Adam(model.parameters(), lr=0.001)

  # Define class mapping
  class_mapping = {
      0: "Workwear",
      1: "Casualwear",
      2: "Partywear"
      # Add more classes as needed
  }

  # Training loop
  num_epochs = 5
  for epoch in range(num_epochs):
      for image, label in dataset:
          inputs = feature_extractor(images=image.unsqueeze(0), return_tensors="pt")
          label = label.squeeze(0)

          # Forward pass
          outputs = model(**inputs)
          logits = outputs.logits

          # Reshape the logits and labels to have the same shape
          logits = logits.view(-1, logits.shape[-1])
          labels = label.view(-1)

          loss = criterion(logits, labels)

          # Backward pass and optimization
          optimizer.zero_grad()
          loss.backward()
          optimizer.step()

          # Print predicted class for each image
          predicted_label = logits.argmax(dim=1)
          predicted_label_name = class_mapping[predicted_label.item()]
          label_name = class_mapping[label.item()]
          print(f"Ground Truth: {label_name}, Predicted: {predicted_label_name}")

      print(f"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}")


# Check if the highest prediction is less than 10%
if top5_prob[0].item() < 0.10:
    print("Our AI model cannot confidently detect your image, possibly due to picture quality or file corruption.")
    user_input = input("Would you like to enter the label manually? (yes/no): ")
    if user_input.lower() == 'yes':
        manual_label = input("Enter the name of the image: ")
        print(f"Manual label entered: {manual_label}")
    else:
        print("No manual label entered.")
else:
    # Interact with the user based on the highest prediction
    predicted_label = class_idx[top5_catid[0].item()]
    highest_percentage = top5_prob[0].item() * 100
    print(f"The top prediction is {predicted_label} with a confidence of {highest_percentage:.2f}%. Is that what you expected?")
    user_response = input("Enter 'yes' if correct: ")

    # Handle user response
    if user_response.lower() == 'yes':
      print("Prediction confirmed.")
      classify_Image(image_path)

    else:
      # User inputs new label
      correct_label = input("Enter the correct label for the image: ")
      print(f"New prediction as per user input: {correct_label}")